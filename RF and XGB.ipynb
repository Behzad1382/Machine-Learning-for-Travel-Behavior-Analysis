{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e6a8993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as SL\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3af9ea43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EXP_1</th>\n",
       "      <th>EXP_2</th>\n",
       "      <th>IMP_1</th>\n",
       "      <th>IMP_2</th>\n",
       "      <th>RES_1</th>\n",
       "      <th>RES_2</th>\n",
       "      <th>FOOBE_1</th>\n",
       "      <th>FOOBE_3</th>\n",
       "      <th>FOOBE_4</th>\n",
       "      <th>FOOBE_9</th>\n",
       "      <th>...</th>\n",
       "      <th>INF_2</th>\n",
       "      <th>INF_3</th>\n",
       "      <th>INF_4</th>\n",
       "      <th>AGE</th>\n",
       "      <th>GEN</th>\n",
       "      <th>DIS</th>\n",
       "      <th>EDU</th>\n",
       "      <th>CWS</th>\n",
       "      <th>HHI</th>\n",
       "      <th>TR1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EXP_1  EXP_2  IMP_1  IMP_2  RES_1  RES_2  FOOBE_1  FOOBE_3  FOOBE_4  \\\n",
       "0      5      5      7      6      2      3        6        7        7   \n",
       "1      5      5      6      5      5      2        7        7        1   \n",
       "2      5      5      5      5      5      3        6        6        6   \n",
       "3      7      7      7      7      7      1        7        7        6   \n",
       "4      5      5      5      4      5      3        4        5        1   \n",
       "\n",
       "   FOOBE_9  ...  INF_2  INF_3  INF_4  AGE  GEN  DIS  EDU  CWS  HHI  TR1  \n",
       "0        6  ...      1      1      2    1    1    2    4    1    8    4  \n",
       "1        6  ...      3      3      3    6    1    2    5    6    6    5  \n",
       "2        6  ...      3      3      4    6    1    2    4    6    3    5  \n",
       "3        5  ...      3      4      4    2    2    3    5    3    4    2  \n",
       "4        1  ...      1      1      1    4    1    1    4    2    4    5  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('C:\\\\Users\\\\monam\\\\OneDrive\\\\Desktop\\\\testt2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c428d811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: OrderedDict([('max_depth', 6), ('max_features', 25), ('min_samples_leaf', 4), ('min_samples_split', 7), ('n_estimators', 110)])\n",
      "Accuracy with best hyperparameters: 0.919047619047619\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a simple dataset\n",
    "# Adjusted n_samples to 1050 and n_features to 32\n",
    "X, y = make_classification(n_samples=1050, n_features=32, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    'n_estimators': Integer(100, 1000),\n",
    "    'max_depth': Integer(1, 50),\n",
    "    'min_samples_split': Integer(2, 10),\n",
    "    'min_samples_leaf': Integer(1, 10),\n",
    "    # Since all your variables are categorical, 'max_features' can be between 1 to n_features\n",
    "    'max_features': Integer(1, 32)\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "opt = BayesSearchCV(\n",
    "    rf,\n",
    "    search_space,\n",
    "    n_iter=30,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Fit the model to find the best hyperparameters\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters found\n",
    "best_params = opt.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "best_rf = RandomForestClassifier(**best_params)\n",
    "best_rf.fit(X_train, y_train)\n",
    "accuracy = best_rf.score(X_test, y_test)\n",
    "print(\"Accuracy with best hyperparameters:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e86e7b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "791363e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c2792ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9190\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "249fddd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.93       112\n",
      "           1       0.96      0.87      0.91        98\n",
      "\n",
      "    accuracy                           0.92       210\n",
      "   macro avg       0.92      0.92      0.92       210\n",
      "weighted avg       0.92      0.92      0.92       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92fee858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "unique_preds = np.unique(y_pred)\n",
    "print(unique_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff347e16",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'read_your_data_method_here'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Assuming your data is loaded into a dataframe named 'data'\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_your_data_method_here()  \u001b[38;5;66;03m# replace with your data loading method\u001b[39;00m\n\u001b[0;32m      9\u001b[0m X \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# All columns except the last one\u001b[39;00m\n\u001b[0;32m     10\u001b[0m y \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'read_your_data_method_here'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming your data is loaded into a dataframe named 'data'\n",
    "data = pd.read_your_data_method_here()  # replace with your data loading method\n",
    "\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data.iloc[:, -1]   # The last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "717696a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monam\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\monam\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\monam\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.46      0.67      0.54        24\n",
      "           2       0.50      0.10      0.17        20\n",
      "           3       0.50      0.22      0.31        18\n",
      "           4       0.00      0.00      0.00        11\n",
      "           5       0.79      0.94      0.86       137\n",
      "\n",
      "    accuracy                           0.72       210\n",
      "   macro avg       0.45      0.39      0.38       210\n",
      "weighted avg       0.66      0.72      0.67       210\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('C:\\\\Users\\\\monam\\\\OneDrive\\\\Desktop\\\\testt2.csv')\n",
    "\n",
    "# Splitting the data into features (X) and target variable (y)\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=110, max_depth=6, max_features=25, min_samples_leaf=4, min_samples_split=7, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plotting a confusion matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "plt.imshow(confusion, cmap=plt.cm.Blues, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "tick_marks = np.arange(len(df.iloc[:, -1].unique()))\n",
    "plt.xticks(tick_marks, df.iloc[:, -1].unique(), rotation=45)\n",
    "plt.yticks(tick_marks, df.iloc[:, -1].unique())\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfd333e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.62      0.52        24\n",
      "           2       0.44      0.20      0.28        20\n",
      "           3       0.25      0.39      0.30        18\n",
      "           4       0.09      0.09      0.09        11\n",
      "           5       0.91      0.85      0.88       137\n",
      "\n",
      "    accuracy                           0.69       210\n",
      "   macro avg       0.43      0.43      0.41       210\n",
      "weighted avg       0.72      0.69      0.69       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights\n",
    "from sklearn.utils import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=[1, 2, 3, 4, 5], y=y_train)\n",
    "weights = {1: class_weights[0], 2: class_weights[1], 3: class_weights[2], 4: class_weights[3], 5: class_weights[4]}\n",
    "\n",
    "# Initialize the RandomForestClassifier with class weights\n",
    "clf_weighted = RandomForestClassifier(n_estimators=110, max_depth=6, max_features=25, min_samples_leaf=4, min_samples_split=7, random_state=42, class_weight=weights)\n",
    "\n",
    "# Fit the model\n",
    "clf_weighted.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred_weighted = clf_weighted.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f6a1f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.58      0.49        24\n",
      "           2       0.14      0.10      0.12        20\n",
      "           3       0.21      0.22      0.22        18\n",
      "           4       0.06      0.09      0.07        11\n",
      "           5       0.83      0.77      0.80       137\n",
      "\n",
      "    accuracy                           0.60       210\n",
      "   macro avg       0.33      0.35      0.34       210\n",
      "weighted avg       0.63      0.60      0.61       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Using SMOTE to oversample the minority classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Training RandomForest on resampled data\n",
    "clf_resampled = RandomForestClassifier(n_estimators=110, max_depth=6, max_features=25, min_samples_leaf=4, min_samples_split=7, random_state=42)\n",
    "clf_resampled.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predicting on the test data\n",
    "y_pred_resampled = clf_resampled.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8172117",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create and train the model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m xgb_classifier \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti:softprob\u001b[39m\u001b[38;5;124m\"\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create and train the model\n",
    "xgb_classifier = xgb.XGBClassifier(objective=\"multi:softprob\", eval_metric=\"mlogloss\")\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "xgb_predictions = xgb_classifier.predict(X_test)\n",
    "print(classification_report(y_test, xgb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "101dc3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Obtaining dependency information for xgboost from https://files.pythonhosted.org/packages/11/6f/419545a6a344cfd1358a80c36a06431881d607830483ef63d7c38905cd22/xgboost-2.0.1-py3-none-win_amd64.whl.metadata\n",
      "  Downloading xgboost-2.0.1-py3-none-win_amd64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\monam\\anaconda3\\lib\\site-packages (from xgboost) (1.26.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\monam\\anaconda3\\lib\\site-packages (from xgboost) (1.11.1)\n",
      "Downloading xgboost-2.0.1-py3-none-win_amd64.whl (99.7 MB)\n",
      "   ---------------------------------------- 0.0/99.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/99.7 MB 660.6 kB/s eta 0:02:31\n",
      "   ---------------------------------------- 0.1/99.7 MB 544.7 kB/s eta 0:03:04\n",
      "   ---------------------------------------- 0.1/99.7 MB 901.1 kB/s eta 0:01:51\n",
      "   ---------------------------------------- 0.3/99.7 MB 1.5 MB/s eta 0:01:08\n",
      "   ---------------------------------------- 0.6/99.7 MB 2.5 MB/s eta 0:00:40\n",
      "   ---------------------------------------- 0.9/99.7 MB 3.0 MB/s eta 0:00:33\n",
      "   ---------------------------------------- 1.2/99.7 MB 3.5 MB/s eta 0:00:29\n",
      "    --------------------------------------- 1.7/99.7 MB 4.4 MB/s eta 0:00:23\n",
      "    --------------------------------------- 2.1/99.7 MB 5.0 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 2.6/99.7 MB 5.5 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 3.0/99.7 MB 5.8 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 3.6/99.7 MB 6.3 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 4.1/99.7 MB 6.7 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 4.6/99.7 MB 7.0 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 5.2/99.7 MB 7.3 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 5.7/99.7 MB 7.7 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 6.0/99.7 MB 7.5 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 6.5/99.7 MB 7.7 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 7.0/99.7 MB 7.9 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 7.5/99.7 MB 8.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 8.0/99.7 MB 8.2 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 8.6/99.7 MB 8.3 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 9.0/99.7 MB 8.3 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 9.6/99.7 MB 8.5 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 10.0/99.7 MB 8.5 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 10.4/99.7 MB 9.8 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 10.4/99.7 MB 9.8 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 10.7/99.7 MB 9.4 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 11.2/99.7 MB 9.8 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 11.7/99.7 MB 9.8 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 12.1/99.7 MB 9.8 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 12.5/99.7 MB 9.6 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 12.7/99.7 MB 9.6 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 12.7/99.7 MB 9.6 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 12.7/99.7 MB 9.6 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 12.7/99.7 MB 9.6 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 12.7/99.7 MB 9.6 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 12.7/99.7 MB 9.6 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 12.7/99.7 MB 9.6 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 12.7/99.7 MB 9.6 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 12.7/99.7 MB 9.6 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 12.7/99.7 MB 9.6 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 12.7/99.7 MB 9.6 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 12.7/99.7 MB 9.6 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 12.7/99.7 MB 9.6 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 12.7/99.7 MB 9.6 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 12.7/99.7 MB 9.6 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 12.7/99.7 MB 5.7 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 12.7/99.7 MB 5.6 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 12.7/99.7 MB 5.6 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 12.8/99.7 MB 5.5 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 13.0/99.7 MB 5.2 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 13.1/99.7 MB 5.2 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 13.5/99.7 MB 5.1 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 13.7/99.7 MB 5.0 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 14.2/99.7 MB 5.0 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 14.2/99.7 MB 5.0 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 14.2/99.7 MB 5.0 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 14.2/99.7 MB 5.0 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 14.5/99.7 MB 4.7 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 15.1/99.7 MB 4.7 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 15.4/99.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 15.9/99.7 MB 4.7 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 16.1/99.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 16.3/99.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 16.3/99.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 16.3/99.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 16.3/99.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 16.3/99.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 16.3/99.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 16.3/99.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 16.3/99.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 16.3/99.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 16.3/99.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 16.3/99.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 16.3/99.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 16.3/99.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 16.3/99.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 16.3/99.7 MB 3.5 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 16.4/99.7 MB 3.5 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 17.1/99.7 MB 3.5 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 17.1/99.7 MB 3.5 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 17.1/99.7 MB 3.5 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 17.1/99.7 MB 3.5 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 17.4/99.7 MB 3.3 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 17.8/99.7 MB 3.3 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 18.3/99.7 MB 3.3 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 18.7/99.7 MB 3.3 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 19.2/99.7 MB 3.3 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 19.7/99.7 MB 3.3 MB/s eta 0:00:25\n",
      "   -------- ------------------------------- 20.2/99.7 MB 3.3 MB/s eta 0:00:25\n",
      "   -------- ------------------------------- 20.6/99.7 MB 3.4 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 20.8/99.7 MB 3.3 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 21.4/99.7 MB 3.3 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 22.0/99.7 MB 3.4 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 22.4/99.7 MB 3.4 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 22.9/99.7 MB 3.4 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 23.2/99.7 MB 4.8 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 23.5/99.7 MB 4.8 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 24.0/99.7 MB 4.9 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 24.2/99.7 MB 4.8 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 24.6/99.7 MB 5.2 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 25.1/99.7 MB 5.2 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 25.7/99.7 MB 5.2 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 25.9/99.7 MB 5.2 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 26.0/99.7 MB 5.1 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 26.0/99.7 MB 5.1 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 26.0/99.7 MB 5.1 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 26.0/99.7 MB 5.1 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 26.0/99.7 MB 5.1 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 26.0/99.7 MB 5.1 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 26.0/99.7 MB 5.1 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 26.0/99.7 MB 5.1 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 26.0/99.7 MB 5.1 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 26.0/99.7 MB 5.1 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 26.0/99.7 MB 5.1 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 26.0/99.7 MB 5.1 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 26.0/99.7 MB 5.1 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 26.0/99.7 MB 5.1 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 26.4/99.7 MB 3.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 26.9/99.7 MB 5.5 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 27.2/99.7 MB 5.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 27.5/99.7 MB 5.7 MB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 27.8/99.7 MB 5.7 MB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 28.1/99.7 MB 5.7 MB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 28.3/99.7 MB 5.6 MB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 28.5/99.7 MB 5.5 MB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 28.6/99.7 MB 5.4 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 28.8/99.7 MB 5.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 28.8/99.7 MB 5.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 28.8/99.7 MB 5.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 28.8/99.7 MB 5.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 28.8/99.7 MB 5.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 28.8/99.7 MB 5.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 28.8/99.7 MB 5.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 28.8/99.7 MB 5.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 28.8/99.7 MB 5.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 28.8/99.7 MB 5.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 28.8/99.7 MB 5.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 28.8/99.7 MB 5.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 28.8/99.7 MB 5.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 28.8/99.7 MB 5.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 28.8/99.7 MB 5.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 28.8/99.7 MB 3.9 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 28.9/99.7 MB 3.9 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 29.4/99.7 MB 3.9 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 29.7/99.7 MB 3.8 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 29.9/99.7 MB 3.8 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 30.1/99.7 MB 3.8 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 30.3/99.7 MB 3.7 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 30.5/99.7 MB 3.7 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 30.8/99.7 MB 3.6 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 31.0/99.7 MB 3.7 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 31.2/99.7 MB 3.6 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 31.5/99.7 MB 3.6 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 31.7/99.7 MB 3.5 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 31.9/99.7 MB 3.5 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 32.1/99.7 MB 3.5 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 32.4/99.7 MB 3.4 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 32.6/99.7 MB 3.4 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 32.8/99.7 MB 3.4 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 33.1/99.7 MB 3.4 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 33.3/99.7 MB 3.3 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 33.6/99.7 MB 3.4 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 33.8/99.7 MB 3.3 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 34.1/99.7 MB 3.3 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 34.3/99.7 MB 3.3 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 34.5/99.7 MB 3.3 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 34.6/99.7 MB 3.3 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 34.7/99.7 MB 3.2 MB/s eta 0:00:21\n",
      "   ------------- -------------------------- 34.9/99.7 MB 3.2 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 35.1/99.7 MB 3.1 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 35.4/99.7 MB 3.1 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 35.7/99.7 MB 3.1 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 35.9/99.7 MB 3.1 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 36.1/99.7 MB 3.1 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 36.3/99.7 MB 3.8 MB/s eta 0:00:17\n",
      "   -------------- ------------------------- 36.5/99.7 MB 3.7 MB/s eta 0:00:17\n",
      "   -------------- ------------------------- 36.8/99.7 MB 3.7 MB/s eta 0:00:17\n",
      "   -------------- ------------------------- 36.9/99.7 MB 3.7 MB/s eta 0:00:18\n",
      "   -------------- ------------------------- 37.1/99.7 MB 3.6 MB/s eta 0:00:18\n",
      "   -------------- ------------------------- 37.3/99.7 MB 3.6 MB/s eta 0:00:18\n",
      "   --------------- ------------------------ 37.5/99.7 MB 3.6 MB/s eta 0:00:18\n",
      "   --------------- ------------------------ 37.9/99.7 MB 3.6 MB/s eta 0:00:18\n",
      "   --------------- ------------------------ 38.1/99.7 MB 3.6 MB/s eta 0:00:18\n",
      "   --------------- ------------------------ 38.5/99.7 MB 3.6 MB/s eta 0:00:17\n",
      "   --------------- ------------------------ 38.6/99.7 MB 3.6 MB/s eta 0:00:17\n",
      "   --------------- ------------------------ 38.9/99.7 MB 3.7 MB/s eta 0:00:17\n",
      "   --------------- ------------------------ 39.2/99.7 MB 5.0 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 39.5/99.7 MB 4.9 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 39.7/99.7 MB 4.9 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 39.9/99.7 MB 4.9 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 40.2/99.7 MB 4.8 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 40.5/99.7 MB 4.9 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 40.7/99.7 MB 5.0 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 41.0/99.7 MB 5.0 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 41.1/99.7 MB 4.9 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 41.3/99.7 MB 4.9 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 41.5/99.7 MB 5.0 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 41.8/99.7 MB 4.9 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 42.1/99.7 MB 4.9 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 42.1/99.7 MB 4.9 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 42.2/99.7 MB 4.8 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 42.2/99.7 MB 4.8 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 42.2/99.7 MB 4.8 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 42.2/99.7 MB 4.8 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 42.2/99.7 MB 4.8 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 42.2/99.7 MB 4.8 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 42.2/99.7 MB 4.8 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 42.2/99.7 MB 4.8 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 42.3/99.7 MB 4.1 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 42.6/99.7 MB 4.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 42.8/99.7 MB 4.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 43.0/99.7 MB 4.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 43.3/99.7 MB 4.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 43.3/99.7 MB 4.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 43.5/99.7 MB 4.0 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 43.8/99.7 MB 4.0 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 44.0/99.7 MB 4.0 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 44.2/99.7 MB 4.0 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 44.2/99.7 MB 4.0 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 44.4/99.7 MB 3.9 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 44.6/99.7 MB 3.9 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 44.8/99.7 MB 3.9 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 45.0/99.7 MB 4.0 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 45.2/99.7 MB 4.0 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 45.4/99.7 MB 4.0 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 45.6/99.7 MB 4.0 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 45.9/99.7 MB 3.9 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 46.2/99.7 MB 4.0 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 46.4/99.7 MB 4.0 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 46.6/99.7 MB 4.0 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 46.8/99.7 MB 4.0 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 47.0/99.7 MB 3.9 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 47.2/99.7 MB 4.0 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 47.3/99.7 MB 4.0 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 47.3/99.7 MB 3.9 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 47.5/99.7 MB 3.9 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 47.8/99.7 MB 3.9 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 48.0/99.7 MB 3.9 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 48.3/99.7 MB 3.9 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 48.5/99.7 MB 3.9 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 48.7/99.7 MB 3.8 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 48.8/99.7 MB 3.8 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 49.0/99.7 MB 3.8 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 49.1/99.7 MB 3.8 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 49.3/99.7 MB 3.8 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 49.3/99.7 MB 3.7 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 49.6/99.7 MB 3.7 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 49.8/99.7 MB 3.7 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 50.0/99.7 MB 3.7 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 50.0/99.7 MB 3.7 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 50.3/99.7 MB 3.6 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 50.5/99.7 MB 3.6 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 50.8/99.7 MB 3.6 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 51.0/99.7 MB 3.6 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 51.3/99.7 MB 3.7 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 51.6/99.7 MB 3.7 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 51.8/99.7 MB 3.7 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 52.0/99.7 MB 3.7 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 52.2/99.7 MB 3.6 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 52.3/99.7 MB 3.6 MB/s eta 0:00:14\n",
      "   --------------------- ------------------ 52.5/99.7 MB 4.3 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 52.8/99.7 MB 4.2 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 53.1/99.7 MB 4.2 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 53.3/99.7 MB 4.2 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 53.5/99.7 MB 4.2 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 53.6/99.7 MB 4.2 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 53.9/99.7 MB 4.3 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 54.1/99.7 MB 4.3 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 54.4/99.7 MB 4.3 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 54.7/99.7 MB 4.4 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 54.9/99.7 MB 4.5 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 55.2/99.7 MB 4.5 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 55.4/99.7 MB 4.5 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 55.6/99.7 MB 4.5 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.0/99.7 MB 4.5 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.0/99.7 MB 4.5 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.3/99.7 MB 4.5 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.5/99.7 MB 4.5 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.6/99.7 MB 4.4 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.8/99.7 MB 4.4 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.8/99.7 MB 4.4 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.8/99.7 MB 4.4 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.8/99.7 MB 4.4 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.8/99.7 MB 4.4 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.8/99.7 MB 4.4 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.8/99.7 MB 4.4 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.8/99.7 MB 4.4 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.8/99.7 MB 4.4 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.9/99.7 MB 3.8 MB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 57.1/99.7 MB 3.7 MB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 57.3/99.7 MB 3.7 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 57.5/99.7 MB 3.7 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 57.5/99.7 MB 3.7 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 57.6/99.7 MB 3.8 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 57.9/99.7 MB 3.7 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 58.0/99.7 MB 3.7 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 58.2/99.7 MB 3.7 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 58.3/99.7 MB 3.7 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 58.3/99.7 MB 3.6 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 58.5/99.7 MB 3.6 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 58.7/99.7 MB 3.6 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 58.9/99.7 MB 3.6 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 59.1/99.7 MB 3.6 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 59.3/99.7 MB 3.6 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 59.5/99.7 MB 3.6 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 59.7/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 59.8/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.0/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.2/99.7 MB 3.6 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.4/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.6/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.8/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.9/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.9/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.9/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.9/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.9/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.9/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.9/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.9/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.9/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.9/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.9/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.9/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.9/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.9/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.9/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 60.9/99.7 MB 3.7 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 61.0/99.7 MB 2.9 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 61.0/99.7 MB 2.9 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 61.2/99.7 MB 2.8 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 61.4/99.7 MB 2.8 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 61.5/99.7 MB 2.8 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 61.7/99.7 MB 2.8 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 61.8/99.7 MB 2.8 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 62.0/99.7 MB 2.8 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 62.1/99.7 MB 2.8 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 62.2/99.7 MB 2.7 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 62.4/99.7 MB 2.7 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 62.5/99.7 MB 2.7 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 62.7/99.7 MB 2.7 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 62.8/99.7 MB 2.7 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 63.0/99.7 MB 2.7 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 63.2/99.7 MB 2.7 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 63.4/99.7 MB 2.7 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 63.7/99.7 MB 2.7 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 63.8/99.7 MB 2.7 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 64.0/99.7 MB 2.7 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 64.1/99.7 MB 2.7 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 64.1/99.7 MB 2.7 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 64.2/99.7 MB 2.6 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 64.4/99.7 MB 2.6 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 64.6/99.7 MB 2.6 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 64.7/99.7 MB 2.6 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 64.9/99.7 MB 2.6 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 65.1/99.7 MB 2.6 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 65.4/99.7 MB 2.6 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 65.4/99.7 MB 2.6 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 65.6/99.7 MB 2.5 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 65.8/99.7 MB 2.5 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 65.9/99.7 MB 2.5 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 66.1/99.7 MB 2.5 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 66.4/99.7 MB 2.5 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 66.5/99.7 MB 2.5 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 66.8/99.7 MB 2.6 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 66.8/99.7 MB 2.6 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 66.8/99.7 MB 2.6 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 67.1/99.7 MB 2.8 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 67.2/99.7 MB 2.8 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 67.3/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 67.4/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 67.4/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 67.6/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 67.7/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 67.9/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 68.0/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 68.2/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 68.3/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 68.4/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 68.6/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 68.6/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 68.8/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 68.9/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 69.1/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 69.3/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 69.5/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 69.7/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 69.8/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 70.0/99.7 MB 2.6 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 70.1/99.7 MB 2.7 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 70.2/99.7 MB 2.6 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 70.4/99.7 MB 2.6 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 70.6/99.7 MB 2.6 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 70.7/99.7 MB 2.6 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 70.9/99.7 MB 2.6 MB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 71.0/99.7 MB 2.6 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 71.1/99.7 MB 2.6 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 71.3/99.7 MB 3.2 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 71.5/99.7 MB 3.2 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 71.6/99.7 MB 3.2 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 71.7/99.7 MB 3.2 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 71.8/99.7 MB 3.2 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 72.0/99.7 MB 3.2 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 72.2/99.7 MB 3.2 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 72.4/99.7 MB 3.2 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 72.6/99.7 MB 3.3 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 72.7/99.7 MB 3.3 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 73.0/99.7 MB 3.3 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 73.1/99.7 MB 3.3 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 73.3/99.7 MB 3.3 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 73.6/99.7 MB 3.3 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 73.8/99.7 MB 3.3 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 73.9/99.7 MB 3.3 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 74.0/99.7 MB 3.3 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 74.1/99.7 MB 3.2 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 74.3/99.7 MB 3.3 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 74.5/99.7 MB 3.3 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 74.7/99.7 MB 3.3 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 74.9/99.7 MB 3.3 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 75.0/99.7 MB 3.3 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 75.2/99.7 MB 3.3 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 75.4/99.7 MB 3.3 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 75.6/99.7 MB 3.3 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 75.8/99.7 MB 3.4 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 76.0/99.7 MB 3.4 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 76.2/99.7 MB 3.4 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 76.5/99.7 MB 3.4 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 76.7/99.7 MB 3.4 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 76.9/99.7 MB 3.4 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 77.1/99.7 MB 3.5 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 77.1/99.7 MB 3.5 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 77.4/99.7 MB 3.4 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 77.6/99.7 MB 3.5 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 77.8/99.7 MB 3.6 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 77.9/99.7 MB 3.6 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 78.1/99.7 MB 3.6 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 78.2/99.7 MB 3.6 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 78.5/99.7 MB 3.6 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 78.7/99.7 MB 3.7 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 79.0/99.7 MB 3.8 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 79.1/99.7 MB 3.8 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 79.4/99.7 MB 3.8 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 79.7/99.7 MB 3.9 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 79.9/99.7 MB 3.9 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 80.0/99.7 MB 3.9 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 80.4/99.7 MB 3.9 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 80.6/99.7 MB 4.0 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 80.8/99.7 MB 4.0 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 81.1/99.7 MB 4.0 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 81.2/99.7 MB 4.0 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 81.3/99.7 MB 4.0 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 81.5/99.7 MB 4.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 81.6/99.7 MB 4.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 81.9/99.7 MB 4.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 82.0/99.7 MB 4.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 82.3/99.7 MB 4.2 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 82.5/99.7 MB 4.2 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 82.6/99.7 MB 4.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 82.8/99.7 MB 4.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 83.0/99.7 MB 4.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 83.2/99.7 MB 4.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 83.4/99.7 MB 4.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 83.6/99.7 MB 4.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 83.8/99.7 MB 4.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 84.1/99.7 MB 4.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 84.2/99.7 MB 4.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 84.5/99.7 MB 4.3 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 84.8/99.7 MB 4.3 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 85.0/99.7 MB 4.4 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 85.4/99.7 MB 4.5 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 85.5/99.7 MB 4.4 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 85.7/99.7 MB 4.4 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 85.7/99.7 MB 4.3 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 86.0/99.7 MB 4.3 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 86.1/99.7 MB 4.4 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 86.3/99.7 MB 4.3 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 86.5/99.7 MB 4.3 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 86.8/99.7 MB 4.3 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 87.1/99.7 MB 4.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 87.4/99.7 MB 4.5 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 87.6/99.7 MB 4.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 87.8/99.7 MB 4.5 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 88.2/99.7 MB 4.6 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 88.4/99.7 MB 4.6 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 88.5/99.7 MB 4.6 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 88.8/99.7 MB 4.6 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 89.2/99.7 MB 4.6 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 89.5/99.7 MB 4.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 89.8/99.7 MB 4.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 90.1/99.7 MB 4.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 90.4/99.7 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 90.7/99.7 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 91.1/99.7 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 91.3/99.7 MB 4.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 91.6/99.7 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 91.8/99.7 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 92.1/99.7 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 92.1/99.7 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 92.5/99.7 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 92.9/99.7 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 93.2/99.7 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 93.5/99.7 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 93.8/99.7 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 94.1/99.7 MB 5.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 94.5/99.7 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 94.8/99.7 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.0/99.7 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.2/99.7 MB 5.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.4/99.7 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.8/99.7 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.1/99.7 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.3/99.7 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.6/99.7 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.9/99.7 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 97.1/99.7 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  97.4/99.7 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  97.8/99.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.1/99.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.4/99.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.7/99.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.0/99.7 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.2/99.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.6/99.7 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.7 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.7 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.7 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.7 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 99.7/99.7 MB 5.4 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "054bf2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\monam\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - xgboost\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _py-xgboost-mutex-2.0      |            cpu_0          11 KB  conda-forge\n",
      "    archspec-0.2.1             |     pyhd8ed1ab_1          40 KB  conda-forge\n",
      "    ca-certificates-2023.7.22  |       h56e8100_0         146 KB  conda-forge\n",
      "    certifi-2023.7.22          |     pyhd8ed1ab_0         150 KB  conda-forge\n",
      "    conda-23.9.0               |  py311h1ea47a8_2         1.2 MB  conda-forge\n",
      "    imbalanced-learn-0.11.0    |     pyhd8ed1ab_0         138 KB  conda-forge\n",
      "    libxgboost-1.7.3           |       hd77b12b_0         1.5 MB\n",
      "    openssl-3.0.11             |       h2bbff1b_2         7.4 MB\n",
      "    py-xgboost-1.7.3           |  py311haa95532_0         274 KB\n",
      "    python_abi-3.11            |          2_cp311           5 KB  conda-forge\n",
      "    truststore-0.8.0           |     pyhd8ed1ab_0          20 KB  conda-forge\n",
      "    xgboost-1.7.3              |  py311haa95532_0          13 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        10.9 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _py-xgboost-mutex  conda-forge/win-64::_py-xgboost-mutex-2.0-cpu_0 \n",
      "  archspec           conda-forge/noarch::archspec-0.2.1-pyhd8ed1ab_1 \n",
      "  imbalanced-learn   conda-forge/noarch::imbalanced-learn-0.11.0-pyhd8ed1ab_0 \n",
      "  libxgboost         pkgs/main/win-64::libxgboost-1.7.3-hd77b12b_0 \n",
      "  numpy-base         pkgs/main/win-64::numpy-base-1.24.3-py311hd01c5d8_1 \n",
      "  py-xgboost         pkgs/main/win-64::py-xgboost-1.7.3-py311haa95532_0 \n",
      "  python_abi         conda-forge/win-64::python_abi-3.11-2_cp311 \n",
      "  scikit-learn       pkgs/main/win-64::scikit-learn-1.3.0-py311hf62ec03_0 \n",
      "  truststore         conda-forge/noarch::truststore-0.8.0-pyhd8ed1ab_0 \n",
      "  xgboost            pkgs/main/win-64::xgboost-1.7.3-py311haa95532_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda              pkgs/main::conda-23.7.4-py311haa95532~ --> conda-forge::conda-23.9.0-py311h1ea47a8_2 \n",
      "  openssl                                 3.0.10-h2bbff1b_2 --> 3.0.11-h2bbff1b_2 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2023.08.22~ --> conda-forge::ca-certificates-2023.7.22-h56e8100_0 \n",
      "  certifi            pkgs/main/win-64::certifi-2023.7.22-p~ --> conda-forge/noarch::certifi-2023.7.22-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "openssl-3.0.11       | 7.4 MB    |            |   0% \n",
      "\n",
      "truststore-0.8.0     | 20 KB     |            |   0% \u001b[A\n",
      "\n",
      "\n",
      "libxgboost-1.7.3     | 1.5 MB    |            |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "xgboost-1.7.3        | 13 KB     |            |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2023 | 146 KB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "conda-23.9.0         | 1.2 MB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "py-xgboost-1.7.3     | 274 KB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python_abi-3.11      | 5 KB      |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "certifi-2023.7.22    | 150 KB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "imbalanced-learn-0.1 | 138 KB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "archspec-0.2.1       | 40 KB     |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_py-xgboost-mutex-2. | 11 KB     |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libxgboost-1.7.3     | 1.5 MB    | 1          |   1% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "xgboost-1.7.3        | 13 KB     | ########## | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "xgboost-1.7.3        | 13 KB     | ########## | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libxgboost-1.7.3     | 1.5 MB    | #          |  10% \u001b[A\u001b[A\n",
      "openssl-3.0.11       | 7.4 MB    |            |   0% \n",
      "\n",
      "truststore-0.8.0     | 20 KB     | #######9   |  79% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2023 | 146 KB    | #          |  11% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "truststore-0.8.0     | 20 KB     | ########## | 100% \u001b[A\n",
      "\n",
      "\n",
      "libxgboost-1.7.3     | 1.5 MB    | ##5        |  26% \u001b[A\u001b[A\n",
      "openssl-3.0.11       | 7.4 MB    | 2          |   2% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2023 | 146 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2023 | 146 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libxgboost-1.7.3     | 1.5 MB    | #####7     |  58% \u001b[A\u001b[A\n",
      "openssl-3.0.11       | 7.4 MB    | 8          |   9% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python_abi-3.11      | 5 KB      | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python_abi-3.11      | 5 KB      | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libxgboost-1.7.3     | 1.5 MB    | ########1  |  81% \u001b[A\u001b[A\n",
      "openssl-3.0.11       | 7.4 MB    | #5         |  16% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "certifi-2023.7.22    | 150 KB    | #          |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "openssl-3.0.11       | 7.4 MB    | ##5        |  26% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "certifi-2023.7.22    | 150 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libxgboost-1.7.3     | 1.5 MB    | ########## | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "py-xgboost-1.7.3     | 274 KB    | 5          |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "openssl-3.0.11       | 7.4 MB    | ####1      |  42% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "archspec-0.2.1       | 40 KB     | ###9       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "archspec-0.2.1       | 40 KB     | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "openssl-3.0.11       | 7.4 MB    | #####5     |  56% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "py-xgboost-1.7.3     | 274 KB    | ####6      |  47% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "conda-23.9.0         | 1.2 MB    | 1          |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_py-xgboost-mutex-2. | 11 KB     | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_py-xgboost-mutex-2. | 11 KB     | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "openssl-3.0.11       | 7.4 MB    | #######    |  70% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "py-xgboost-1.7.3     | 274 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "py-xgboost-1.7.3     | 274 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "conda-23.9.0         | 1.2 MB    | #          |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "openssl-3.0.11       | 7.4 MB    | ########3  |  83% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "conda-23.9.0         | 1.2 MB    | ##4        |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "openssl-3.0.11       | 7.4 MB    | #########6 |  97% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "imbalanced-learn-0.1 | 138 KB    | #1         |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "conda-23.9.0         | 1.2 MB    | ########   |  80% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "openssl-3.0.11       | 7.4 MB    | ########## | 100% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "imbalanced-learn-0.1 | 138 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "imbalanced-learn-0.1 | 138 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "conda-23.9.0         | 1.2 MB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "                                                     \n",
      "\n",
      "\n",
      "                                                     \u001b[A\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... \n",
      "\n",
      "    Windows 64-bit packages of scikit-learn can be accelerated using scikit-learn-intelex.\n",
      "    More details are available here: https://intel.github.io/scikit-learn-intelex\n",
      "\n",
      "    For example:\n",
      "\n",
      "        $ conda install scikit-learn-intelex\n",
      "        $ python -m sklearnex my_application.py\n",
      "\n",
      "\n",
      "done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - defaults/win-64::anaconda-catalogs==0.2.0=py311haa95532_0\n",
      "  - defaults/win-64::astropy==5.1=py311h5bb9823_0\n",
      "  - defaults/win-64::bokeh==3.2.1=py311h746a85d_0\n",
      "  - defaults/win-64::bottleneck==1.3.5=py311h5bb9823_0\n",
      "  - defaults/win-64::contourpy==1.0.5=py311h59b6b97_0\n",
      "  - defaults/win-64::daal4py==2023.1.1=py311h30df693_0\n",
      "  - defaults/win-64::dask==2023.6.0=py311haa95532_0\n",
      "  - defaults/win-64::datasets==2.12.0=py311haa95532_0\n",
      "  - defaults/win-64::datashader==0.15.2=py311haa95532_0\n",
      "  - defaults/win-64::datashape==0.5.4=py311haa95532_1\n",
      "  - defaults/win-64::gensim==4.3.0=py311heda8569_0\n",
      "  - defaults/win-64::h5py==3.9.0=py311h4e0e482_0\n",
      "  - defaults/win-64::holoviews==1.17.1=py311haa95532_0\n",
      "  - defaults/win-64::hvplot==0.8.4=py311haa95532_0\n",
      "  - defaults/win-64::imagecodecs==2023.1.23=py311he6ff3c7_0\n",
      "  - defaults/win-64::imageio==2.26.0=py311haa95532_0\n",
      "  - defaults/win-64::intake==0.6.8=py311haa95532_0\n",
      "  - defaults/win-64::matplotlib==3.7.2=py311haa95532_0\n",
      "  - defaults/win-64::matplotlib-base==3.7.2=py311hf62ec03_0\n",
      "  - defaults/win-64::mkl_fft==1.3.8=py311h2bbff1b_0\n",
      "  - defaults/win-64::mkl_random==1.2.4=py311h59b6b97_0\n",
      "  - defaults/win-64::numba==0.57.1=py311hf62ec03_0\n",
      "  - defaults/win-64::numexpr==2.8.4=py311h1fcbade_1\n",
      "  - defaults/win-64::numpy==1.24.3=py311hdab7c0b_1\n",
      "  - defaults/win-64::pandas==2.0.3=py311hf62ec03_0\n",
      "  - defaults/win-64::panel==1.2.3=py311haa95532_0\n",
      "  - defaults/win-64::patsy==0.5.3=py311haa95532_0\n",
      "  - defaults/win-64::pyarrow==11.0.0=py311h8a3a540_1\n",
      "  - defaults/win-64::pyerfa==2.0.0=py311h2bbff1b_0\n",
      "  - defaults/win-64::pytables==3.8.0=py311h4671533_3\n",
      "  - defaults/win-64::pywavelets==1.4.1=py311h2bbff1b_0\n",
      "  - defaults/win-64::scikit-image==0.20.0=py311h3513d60_0\n",
      "  - defaults/win-64::scikit-learn-intelex==2023.1.1=py311haa95532_0\n",
      "  - defaults/win-64::scipy==1.11.1=py311hc1ccb85_0\n",
      "  - defaults/win-64::seaborn==0.12.2=py311haa95532_0\n",
      "  - defaults/win-64::statsmodels==0.14.0=py311hd7041d2_0\n",
      "  - defaults/win-64::tifffile==2023.4.12=py311haa95532_0\n",
      "  - defaults/win-64::transformers==4.32.1=py311haa95532_0\n",
      "  - defaults/win-64::xarray==2023.6.0=py311haa95532_0\n",
      "  - defaults/win-64::_anaconda_depends==2023.09=py311_mkl_1\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 23.9.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.9.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ec93291",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create and train the model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m xgb_classifier \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti:softprob\u001b[39m\u001b[38;5;124m\"\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m xgb_classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Predict and evaluate\u001b[39;00m\n\u001b[0;32m      8\u001b[0m xgb_predictions \u001b[38;5;241m=\u001b[39m xgb_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create and train the model\n",
    "xgb_classifier = xgb.XGBClassifier(objective=\"multi:softprob\", eval_metric=\"mlogloss\")\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "xgb_predictions = xgb_classifier.predict(X_test)\n",
    "print(classification_report(y_test, xgb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0d4ff9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4], got [1 2 3 4 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Create and train the model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m xgb_classifier \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti:softprob\u001b[39m\u001b[38;5;124m\"\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m xgb_classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1440\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1435\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_)\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1437\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1438\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1439\u001b[0m ):\n\u001b[1;32m-> 1440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1442\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1443\u001b[0m     )\n\u001b[0;32m   1445\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4], got [1 2 3 4 5]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('C:\\\\Users\\\\monam\\\\OneDrive\\\\Desktop\\\\testt2.csv')\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "xgb_classifier = xgb.XGBClassifier(objective=\"multi:softprob\", eval_metric=\"mlogloss\")\n",
    "xgb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d89eabd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;,\n",
       "              predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;,\n",
       "              predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='mlogloss',\n",
       "              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, objective='multi:softprob',\n",
       "              predictor=None, ...)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust the class labels\n",
    "y_train_adj = y_train - 1\n",
    "y_test_adj = y_test - 1\n",
    "\n",
    "# Create and train the model\n",
    "xgb_classifier = xgb.XGBClassifier(objective=\"multi:softprob\", eval_metric=\"mlogloss\")\n",
    "xgb_classifier.fit(X_train, y_train_adj)\n",
    "\n",
    "# When you make predictions, remember to adjust them back\n",
    "# xgb_predictions = xgb_classifier.predict(X_test) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b23d426b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4], got [1 2 3 4 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Create and train the model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m xgb_classifier \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti:softprob\u001b[39m\u001b[38;5;124m\"\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m xgb_classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1440\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1435\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_)\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1437\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1438\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1439\u001b[0m ):\n\u001b[1;32m-> 1440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1442\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1443\u001b[0m     )\n\u001b[0;32m   1445\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4], got [1 2 3 4 5]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('C:\\\\Users\\\\monam\\\\OneDrive\\\\Desktop\\\\testt2.csv')\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "xgb_classifier = xgb.XGBClassifier(objective=\"multi:softprob\", eval_metric=\"mlogloss\")\n",
    "xgb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71ce2177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;,\n",
       "              predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;,\n",
       "              predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='mlogloss',\n",
       "              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, objective='multi:softprob',\n",
       "              predictor=None, ...)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('C:\\\\Users\\\\monam\\\\OneDrive\\\\Desktop\\\\testt2.csv')\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Adjust the class labels for XGBoost\n",
    "y_adj = y - 1\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train_adj, y_test_adj = train_test_split(X, y_adj, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "xgb_classifier = xgb.XGBClassifier(objective=\"multi:softprob\", eval_metric=\"mlogloss\")\n",
    "xgb_classifier.fit(X_train, y_train_adj)\n",
    "\n",
    "# When you make predictions, remember to adjust them back\n",
    "# xgb_predictions = xgb_classifier.predict(X_test) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52f6c532",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4], got [1 2 3 4 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create and train the model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m xgb_classifier \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti:softprob\u001b[39m\u001b[38;5;124m\"\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m xgb_classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Predict and evaluate\u001b[39;00m\n\u001b[0;32m      8\u001b[0m xgb_predictions \u001b[38;5;241m=\u001b[39m xgb_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1440\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1435\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_)\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1437\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1438\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1439\u001b[0m ):\n\u001b[1;32m-> 1440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1442\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1443\u001b[0m     )\n\u001b[0;32m   1445\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4], got [1 2 3 4 5]"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create and train the model\n",
    "xgb_classifier = xgb.XGBClassifier(objective=\"multi:softprob\", eval_metric=\"mlogloss\")\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "xgb_predictions = xgb_classifier.predict(X_test)\n",
    "print(classification_report(y_test, xgb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57ab1a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.46      0.52      0.49        42\n",
      "           2       0.26      0.19      0.22        27\n",
      "           3       0.24      0.12      0.16        32\n",
      "           4       0.33      0.12      0.18        16\n",
      "           5       0.82      0.93      0.87       198\n",
      "\n",
      "    accuracy                           0.69       315\n",
      "   macro avg       0.42      0.38      0.39       315\n",
      "weighted avg       0.64      0.69      0.66       315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create and train the model\n",
    "xgb_classifier = xgb.XGBClassifier(objective=\"multi:softprob\", eval_metric=\"mlogloss\")\n",
    "\n",
    "# Adjust the class labels for training\n",
    "y_train_adj = y_train - 1\n",
    "xgb_classifier.fit(X_train, y_train_adj)\n",
    "\n",
    "# Predict \n",
    "xgb_predictions_adj = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Adjust predictions back to original class labels\n",
    "xgb_predictions = xgb_predictions_adj + 1\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, xgb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80eb2e92",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bayes_opt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbayes_opt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BayesianOptimization\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_val_score\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Define the XGBoost function to be optimized by Bayesian Optimization\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bayes_opt'"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the XGBoost function to be optimized by Bayesian Optimization\n",
    "def xgb_evaluate(max_depth, gamma, colsample_bytree, eta):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'gamma': gamma,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'eta': eta,\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 5,\n",
    "        'eval_metric': 'mlogloss'\n",
    "    }\n",
    "    cv_result = xgb.cv(params, dtrain, num_boost_round=100, nfold=3)    \n",
    "    return -1.0 * cv_result['test-mlogloss-mean'].iloc[-1]\n",
    "\n",
    "# Convert training data to DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train-1)\n",
    "\n",
    "# Hyperparameter bounds\n",
    "xgb_bo = BayesianOptimization(xgb_evaluate, {\n",
    "    'max_depth': (3, 10),\n",
    "    'gamma': (0, 1),\n",
    "    'colsample_bytree': (0.3, 1),\n",
    "    'eta': (0.1, 0.3)\n",
    "})\n",
    "\n",
    "# Maximize the accuracy\n",
    "xgb_bo.maximize(init_points=5, n_iter=15)\n",
    "\n",
    "# Extract the parameters of the best iteration\n",
    "params = xgb_bo.max['params']\n",
    "params['max_depth'] = int(params['max_depth'])\n",
    "\n",
    "# Train the model with the best parameters found by Bayesian Optimization\n",
    "xgb_classifier = xgb.XGBClassifier(**params, objective='multi:softprob', num_class=5, eval_metric='mlogloss')\n",
    "xgb_classifier.fit(X_train, y_train-1)\n",
    "\n",
    "# Predict \n",
    "xgb_predictions_adj = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Adjust predictions back to original class labels\n",
    "xgb_predictions = xgb_predictions_adj + 1\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, xgb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60941052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian-optimization\n",
      "  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\monam\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\monam\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\users\\monam\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.3.0)\n",
      "Requirement already satisfied: colorama>=0.4.6 in c:\\users\\monam\\anaconda3\\lib\\site-packages (from bayesian-optimization) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\monam\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\monam\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (2.2.0)\n",
      "Installing collected packages: bayesian-optimization\n",
      "Successfully installed bayesian-optimization-1.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e682262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as SL\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b12aee9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EXP_1</th>\n",
       "      <th>EXP_2</th>\n",
       "      <th>IMP_1</th>\n",
       "      <th>IMP_2</th>\n",
       "      <th>RES_1</th>\n",
       "      <th>RES_2</th>\n",
       "      <th>FOOBE_1</th>\n",
       "      <th>FOOBE_3</th>\n",
       "      <th>FOOBE_4</th>\n",
       "      <th>FOOBE_9</th>\n",
       "      <th>...</th>\n",
       "      <th>INF_2</th>\n",
       "      <th>INF_3</th>\n",
       "      <th>INF_4</th>\n",
       "      <th>AGE</th>\n",
       "      <th>GEN</th>\n",
       "      <th>DIS</th>\n",
       "      <th>EDU</th>\n",
       "      <th>CWS</th>\n",
       "      <th>HHI</th>\n",
       "      <th>TR1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EXP_1  EXP_2  IMP_1  IMP_2  RES_1  RES_2  FOOBE_1  FOOBE_3  FOOBE_4  \\\n",
       "0      5      5      7      6      2      3        6        7        7   \n",
       "1      5      5      6      5      5      2        7        7        1   \n",
       "2      5      5      5      5      5      3        6        6        6   \n",
       "3      7      7      7      7      7      1        7        7        6   \n",
       "4      5      5      5      4      5      3        4        5        1   \n",
       "\n",
       "   FOOBE_9  ...  INF_2  INF_3  INF_4  AGE  GEN  DIS  EDU  CWS  HHI  TR1  \n",
       "0        6  ...      1      1      2    1    1    2    4    1    8    4  \n",
       "1        6  ...      3      3      3    6    1    2    5    6    6    5  \n",
       "2        6  ...      3      3      4    6    1    2    4    6    3    5  \n",
       "3        5  ...      3      4      4    2    2    3    5    3    4    2  \n",
       "4        1  ...      1      1      1    4    1    1    4    2    4    5  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('C:\\\\Users\\\\monam\\\\OneDrive\\\\Desktop\\\\testt2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dafec56",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m*\u001b[39m cv_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest-mlogloss-mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Convert training data to DMatrix for XGBoost\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m dtrain \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(X_train, label\u001b[38;5;241m=\u001b[39my_train\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Hyperparameter bounds\u001b[39;00m\n\u001b[0;32m     24\u001b[0m xgb_bo \u001b[38;5;241m=\u001b[39m BayesianOptimization(xgb_evaluate, {\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meta\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.3\u001b[39m)\n\u001b[0;32m     29\u001b[0m })\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the XGBoost function to be optimized by Bayesian Optimization\n",
    "def xgb_evaluate(max_depth, gamma, colsample_bytree, eta):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'gamma': gamma,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'eta': eta,\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 5,\n",
    "        'eval_metric': 'mlogloss'\n",
    "    }\n",
    "    cv_result = xgb.cv(params, dtrain, num_boost_round=100, nfold=3)    \n",
    "    return -1.0 * cv_result['test-mlogloss-mean'].iloc[-1]\n",
    "\n",
    "# Convert training data to DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train-1)\n",
    "\n",
    "# Hyperparameter bounds\n",
    "xgb_bo = BayesianOptimization(xgb_evaluate, {\n",
    "    'max_depth': (3, 10),\n",
    "    'gamma': (0, 1),\n",
    "    'colsample_bytree': (0.3, 1),\n",
    "    'eta': (0.1, 0.3)\n",
    "})\n",
    "\n",
    "# Maximize the accuracy\n",
    "xgb_bo.maximize(init_points=5, n_iter=15)\n",
    "\n",
    "# Extract the parameters of the best iteration\n",
    "params = xgb_bo.max['params']\n",
    "params['max_depth'] = int(params['max_depth'])\n",
    "\n",
    "# Train the model with the best parameters found by Bayesian Optimization\n",
    "xgb_classifier = xgb.XGBClassifier(**params, objective='multi:softprob', num_class=5, eval_metric='mlogloss')\n",
    "xgb_classifier.fit(X_train, y_train-1)\n",
    "\n",
    "# Predict \n",
    "xgb_predictions_adj = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Adjust predictions back to original class labels\n",
    "xgb_predictions = xgb_predictions_adj + 1\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, xgb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755677e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('C:\\\\Users\\\\monam\\\\OneDrive\\\\Desktop\\\\testt2.csv')\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5042723f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |    eta    |   gamma   | max_depth |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-0.8851  \u001b[0m | \u001b[0m0.3491   \u001b[0m | \u001b[0m0.2202   \u001b[0m | \u001b[0m0.7392   \u001b[0m | \u001b[0m3.526    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m-0.9434  \u001b[0m | \u001b[0m0.8339   \u001b[0m | \u001b[0m0.1217   \u001b[0m | \u001b[0m0.2908   \u001b[0m | \u001b[0m5.728    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m-1.031   \u001b[0m | \u001b[0m0.6528   \u001b[0m | \u001b[0m0.2621   \u001b[0m | \u001b[0m0.06243  \u001b[0m | \u001b[0m9.322    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-0.8958  \u001b[0m | \u001b[0m0.6292   \u001b[0m | \u001b[0m0.1619   \u001b[0m | \u001b[0m0.6823   \u001b[0m | \u001b[0m4.157    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-0.8992  \u001b[0m | \u001b[0m0.9054   \u001b[0m | \u001b[0m0.1386   \u001b[0m | \u001b[0m0.7634   \u001b[0m | \u001b[0m6.886    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-0.9068  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m3.0      \u001b[0m |\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m-0.8801  \u001b[0m | \u001b[95m0.9676   \u001b[0m | \u001b[95m0.2749   \u001b[0m | \u001b[95m0.9903   \u001b[0m | \u001b[95m3.107    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-0.8807  \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m7.404    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-1.066   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m7.148    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-0.9179  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m0.4751   \u001b[0m | \u001b[0m3.604    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-0.8863  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m7.722    \u001b[0m |\n",
      "| \u001b[95m12       \u001b[0m | \u001b[95m-0.8751  \u001b[0m | \u001b[95m0.3      \u001b[0m | \u001b[95m0.1      \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m5.013    \u001b[0m |\n",
      "| \u001b[95m13       \u001b[0m | \u001b[95m-0.8712  \u001b[0m | \u001b[95m0.3      \u001b[0m | \u001b[95m0.1      \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m6.143    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m-0.8853  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m5.616    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-0.8793  \u001b[0m | \u001b[0m0.391    \u001b[0m | \u001b[0m0.1979   \u001b[0m | \u001b[0m0.9975   \u001b[0m | \u001b[0m3.024    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m-0.897   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m6.427    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m-1.038   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m3.0      \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m-0.8893  \u001b[0m | \u001b[0m0.9697   \u001b[0m | \u001b[0m0.2557   \u001b[0m | \u001b[0m0.9939   \u001b[0m | \u001b[0m4.757    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m-0.9012  \u001b[0m | \u001b[0m0.3715   \u001b[0m | \u001b[0m0.2231   \u001b[0m | \u001b[0m0.4415   \u001b[0m | \u001b[0m8.116    \u001b[0m |\n",
      "| \u001b[95m20       \u001b[0m | \u001b[95m-0.8705  \u001b[0m | \u001b[95m0.3      \u001b[0m | \u001b[95m0.1      \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m8.244    \u001b[0m |\n",
      "=========================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.48      0.57      0.52        42\n",
      "           2       0.23      0.11      0.15        27\n",
      "           3       0.36      0.12      0.19        32\n",
      "           4       0.00      0.00      0.00        16\n",
      "           5       0.78      0.94      0.86       198\n",
      "\n",
      "    accuracy                           0.69       315\n",
      "   macro avg       0.37      0.35      0.34       315\n",
      "weighted avg       0.61      0.69      0.64       315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the XGBoost function to be optimized by Bayesian Optimization\n",
    "def xgb_evaluate(max_depth, gamma, colsample_bytree, eta):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'gamma': gamma,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'eta': eta,\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 5,\n",
    "        'eval_metric': 'mlogloss'\n",
    "    }\n",
    "    cv_result = xgb.cv(params, dtrain, num_boost_round=100, nfold=3)    \n",
    "    return -1.0 * cv_result['test-mlogloss-mean'].iloc[-1]\n",
    "\n",
    "# Convert training data to DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train-1)\n",
    "\n",
    "# Hyperparameter bounds\n",
    "xgb_bo = BayesianOptimization(xgb_evaluate, {\n",
    "    'max_depth': (3, 10),\n",
    "    'gamma': (0, 1),\n",
    "    'colsample_bytree': (0.3, 1),\n",
    "    'eta': (0.1, 0.3)\n",
    "})\n",
    "\n",
    "# Maximize the accuracy\n",
    "xgb_bo.maximize(init_points=5, n_iter=15)\n",
    "\n",
    "# Extract the parameters of the best iteration\n",
    "params = xgb_bo.max['params']\n",
    "params['max_depth'] = int(params['max_depth'])\n",
    "\n",
    "# Train the model with the best parameters found by Bayesian Optimization\n",
    "xgb_classifier = xgb.XGBClassifier(**params, objective='multi:softprob', num_class=5, eval_metric='mlogloss')\n",
    "xgb_classifier.fit(X_train, y_train-1)\n",
    "\n",
    "# Predict \n",
    "xgb_predictions_adj = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Adjust predictions back to original class labels\n",
    "xgb_predictions = xgb_predictions_adj + 1\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, xgb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a072b3aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4], got [1 2 3 4 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Now, train the XGBoost classifier on the resampled dataset\u001b[39;00m\n\u001b[0;32m     10\u001b[0m xgb_classifier \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti:softprob\u001b[39m\u001b[38;5;124m\"\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m xgb_classifier\u001b[38;5;241m.\u001b[39mfit(X_resampled, y_resampled)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1440\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1435\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_)\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1437\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1438\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1439\u001b[0m ):\n\u001b[1;32m-> 1440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1442\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1443\u001b[0m     )\n\u001b[0;32m   1445\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4], got [1 2 3 4 5]"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Create a SMOTE instance\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Resample the dataset\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Now, train the XGBoost classifier on the resampled dataset\n",
    "xgb_classifier = xgb.XGBClassifier(objective=\"multi:softprob\", eval_metric=\"mlogloss\")\n",
    "xgb_classifier.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a49808ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;,\n",
       "              predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;,\n",
       "              predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='mlogloss',\n",
       "              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, objective='multi:softprob',\n",
       "              predictor=None, ...)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust the labels\n",
    "y_resampled_adjusted = y_resampled - 1\n",
    "\n",
    "# Train the XGBoost classifier with adjusted labels\n",
    "xgb_classifier = xgb.XGBClassifier(objective=\"multi:softprob\", eval_metric=\"mlogloss\")\n",
    "xgb_classifier.fit(X_resampled, y_resampled_adjusted)\n",
    "\n",
    "# When predicting or evaluating, if you need the original labels, simply add 1 to the predictions:\n",
    "# predictions = xgb_classifier.predict(X_test) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fb858e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;,\n",
       "              predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;,\n",
       "              predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='mlogloss',\n",
       "              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, objective='multi:softprob',\n",
       "              predictor=None, ...)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Create a SMOTE instance\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Adjust the labels\n",
    "y_train_adjusted = y_train - 1\n",
    "\n",
    "# Resample the dataset\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train_adjusted)\n",
    "\n",
    "# Now, train the XGBoost classifier on the resampled dataset\n",
    "xgb_classifier = xgb.XGBClassifier(objective=\"multi:softprob\", eval_metric=\"mlogloss\")\n",
    "xgb_classifier.fit(X_resampled, y_resampled)\n",
    "\n",
    "# When predicting or evaluating, if you need the original labels, simply add 1 to the predictions:\n",
    "# predictions = xgb_classifier.predict(X_test) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd2b458c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |    eta    |   gamma   | max_depth |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-0.8768  \u001b[0m | \u001b[0m0.516    \u001b[0m | \u001b[0m0.1611   \u001b[0m | \u001b[0m0.9646   \u001b[0m | \u001b[0m8.928    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m-0.9344  \u001b[0m | \u001b[0m0.3048   \u001b[0m | \u001b[0m0.2034   \u001b[0m | \u001b[0m0.3161   \u001b[0m | \u001b[0m5.888    \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m-0.8715  \u001b[0m | \u001b[95m0.6936   \u001b[0m | \u001b[95m0.1125   \u001b[0m | \u001b[95m0.9264   \u001b[0m | \u001b[95m9.673    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-0.9202  \u001b[0m | \u001b[0m0.3463   \u001b[0m | \u001b[0m0.1912   \u001b[0m | \u001b[0m0.2489   \u001b[0m | \u001b[0m9.464    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-0.9962  \u001b[0m | \u001b[0m0.5503   \u001b[0m | \u001b[0m0.2256   \u001b[0m | \u001b[0m0.08602  \u001b[0m | \u001b[0m7.664    \u001b[0m |\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m-0.8566  \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m0.1      \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m3.0      \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-1.078   \u001b[0m | \u001b[0m0.5336   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m3.0      \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-0.894   \u001b[0m | \u001b[0m0.9714   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m0.9189   \u001b[0m | \u001b[0m9.241    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-0.8827  \u001b[0m | \u001b[0m0.465    \u001b[0m | \u001b[0m0.1746   \u001b[0m | \u001b[0m0.9819   \u001b[0m | \u001b[0m8.903    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-0.8566  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m3.786    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-0.8617  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m4.698    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-0.8724  \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m4.33     \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m-0.9523  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m0.03181  \u001b[0m | \u001b[0m4.734    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m-0.8853  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m5.604    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-0.9071  \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m5.102    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m-0.8822  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m6.634    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m-0.8669  \u001b[0m | \u001b[0m0.337    \u001b[0m | \u001b[0m0.1624   \u001b[0m | \u001b[0m0.9978   \u001b[0m | \u001b[0m3.495    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m-1.015   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m-0.879   \u001b[0m | \u001b[0m0.3056   \u001b[0m | \u001b[0m0.2617   \u001b[0m | \u001b[0m0.9978   \u001b[0m | \u001b[0m9.997    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m-0.8876  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m8.255    \u001b[0m |\n",
      "=========================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.52      0.69      0.59        42\n",
      "           2       0.46      0.22      0.30        27\n",
      "           3       0.43      0.19      0.26        32\n",
      "           4       0.00      0.00      0.00        16\n",
      "           5       0.81      0.93      0.87       198\n",
      "\n",
      "    accuracy                           0.72       315\n",
      "   macro avg       0.44      0.41      0.40       315\n",
      "weighted avg       0.66      0.72      0.68       315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the XGBoost function to be optimized by Bayesian Optimization\n",
    "def xgb_evaluate(max_depth, gamma, colsample_bytree, eta):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'gamma': gamma,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'eta': eta,\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 5,\n",
    "        'eval_metric': 'mlogloss'\n",
    "    }\n",
    "    cv_result = xgb.cv(params, dtrain, num_boost_round=100, nfold=3)    \n",
    "    return -1.0 * cv_result['test-mlogloss-mean'].iloc[-1]\n",
    "\n",
    "# Convert training data to DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train-1)\n",
    "\n",
    "# Hyperparameter bounds\n",
    "xgb_bo = BayesianOptimization(xgb_evaluate, {\n",
    "    'max_depth': (3, 10),\n",
    "    'gamma': (0, 1),\n",
    "    'colsample_bytree': (0.3, 1),\n",
    "    'eta': (0.1, 0.3)\n",
    "})\n",
    "\n",
    "# Maximize the accuracy\n",
    "xgb_bo.maximize(init_points=5, n_iter=15)\n",
    "\n",
    "# Extract the parameters of the best iteration\n",
    "params = xgb_bo.max['params']\n",
    "params['max_depth'] = int(params['max_depth'])\n",
    "\n",
    "# Train the model with the best parameters found by Bayesian Optimization\n",
    "xgb_classifier = xgb.XGBClassifier(**params, objective='multi:softprob', num_class=5, eval_metric='mlogloss')\n",
    "xgb_classifier.fit(X_train, y_train-1)\n",
    "\n",
    "# Predict \n",
    "xgb_predictions_adj = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Adjust predictions back to original class labels\n",
    "xgb_predictions = xgb_predictions_adj + 1\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, xgb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "503f24a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;,\n",
       "              predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;,\n",
       "              predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='mlogloss',\n",
       "              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, objective='multi:softprob',\n",
       "              predictor=None, ...)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Create a SMOTE instance\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Adjust the labels\n",
    "y_train_adjusted = y_train - 1\n",
    "\n",
    "# Resample the dataset\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train_adjusted)\n",
    "\n",
    "# Now, train the XGBoost classifier on the resampled dataset\n",
    "xgb_classifier = xgb.XGBClassifier(objective=\"multi:softprob\", eval_metric=\"mlogloss\")\n",
    "xgb_classifier.fit(X_resampled, y_resampled)\n",
    "\n",
    "# When predicting or evaluating, if you need the original labels, simply add 1 to the predictions:\n",
    "# predictions = xgb_classifier.predict(X_test) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a36756e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4], got [1 2 3 4 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Create and train the model\u001b[39;00m\n\u001b[0;32m     15\u001b[0m xgb_classifier \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti:softprob\u001b[39m\u001b[38;5;124m\"\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m xgb_classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Predict and evaluate\u001b[39;00m\n\u001b[0;32m     19\u001b[0m xgb_predictions \u001b[38;5;241m=\u001b[39m xgb_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1440\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1435\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_)\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1437\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1438\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1439\u001b[0m ):\n\u001b[1;32m-> 1440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1442\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1443\u001b[0m     )\n\u001b[0;32m   1445\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4], got [1 2 3 4 5]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('C:\\\\Users\\\\monam\\\\OneDrive\\\\Desktop\\\\testt2.csv')\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "xgb_classifier = xgb.XGBClassifier(objective=\"multi:softprob\", eval_metric=\"mlogloss\")\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "xgb_predictions = xgb_classifier.predict(X_test)\n",
    "print(classification_report(y_test, xgb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69e62770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.52      0.49        42\n",
      "           1       0.26      0.19      0.22        27\n",
      "           2       0.24      0.12      0.16        32\n",
      "           3       0.33      0.12      0.18        16\n",
      "           4       0.82      0.93      0.87       198\n",
      "\n",
      "    accuracy                           0.69       315\n",
      "   macro avg       0.42      0.38      0.39       315\n",
      "weighted avg       0.64      0.69      0.66       315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('C:\\\\Users\\\\monam\\\\OneDrive\\\\Desktop\\\\testt2.csv')\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Encode the target variable to have class labels starting from 0\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "xgb_classifier = xgb.XGBClassifier(objective=\"multi:softprob\", eval_metric=\"mlogloss\")\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "xgb_predictions = xgb_classifier.predict(X_test)\n",
    "print(classification_report(y_test, xgb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05627a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6095238095238096\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.33      0.35        42\n",
      "           1       0.09      0.04      0.05        27\n",
      "           2       0.29      0.06      0.10        32\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.71      0.88      0.79       198\n",
      "\n",
      "    accuracy                           0.61       315\n",
      "   macro avg       0.29      0.26      0.26       315\n",
      "weighted avg       0.53      0.61      0.56       315\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monam\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you already have your dataset loaded and preprocessed\n",
    "# X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "# Normalize your input features (optional but often recommended)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the MLPClassifier (feedforward neural network)\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),  # Specify the architecture (adjust as needed)\n",
    "    activation='relu',            # Activation function for hidden layers\n",
    "    solver='adam',                # Optimizer\n",
    "    alpha=0.0001,                 # L2 penalty (regularization term)\n",
    "    max_iter=100,                 # Maximum number of iterations\n",
    "    random_state=42,              # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = mlp.score(X_test, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Generate a classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cfa037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    max_iter=1000,  # Increase the number of iterations\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c96aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    max_iter=1000,\n",
    "    learning_rate_init=0.001,  # Adjust the learning rate\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e50695a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    max_iter=1000,\n",
    "    early_stopping=True,  # Enable early stopping\n",
    "    validation_fraction=0.1,  # Fraction of training data to use for validation\n",
    "    n_iter_no_change=20,  # Number of iterations with no improvement on the validation set to wait\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9337d524",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This MLPClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use the trained MLPClassifier to make predictions on the test data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the model's performance on the test data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, accuracy_score, confusion_matrix\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1159\u001b[0m, in \u001b[0;36mMLPClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict using the multi-layer perceptron classifier.\u001b[39;00m\n\u001b[0;32m   1148\u001b[0m \n\u001b[0;32m   1149\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1159\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1462\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[0;32m   1461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This MLPClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# Use the trained MLPClassifier to make predictions on the test data\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Generate a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d8ffecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the MLPClassifier model\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained MLPClassifier to make predictions on the test data\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Now, you can evaluate the model's performance as shown in the previous code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "591ad8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5523809523809524\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.24      0.25        42\n",
      "           1       0.09      0.07      0.08        27\n",
      "           2       0.26      0.19      0.22        32\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.70      0.79      0.74       198\n",
      "\n",
      "    accuracy                           0.55       315\n",
      "   macro avg       0.26      0.26      0.26       315\n",
      "weighted avg       0.51      0.55      0.53       315\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 10   4   5   0  23]\n",
      " [  6   2   3   0  16]\n",
      " [  6   3   6   1  16]\n",
      " [  2   1   2   0  11]\n",
      " [ 15  12   7   8 156]]\n"
     ]
    }
   ],
   "source": [
    "# Use the trained MLPClassifier to make predictions on the test data\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Generate a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa9dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
